{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
    "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
    "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
    "\n",
    "\n",
    "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "|Mountain (+)|Forest (-)|\n",
    "|---|---|\n",
    "|![](./data/mountain/art1131.jpg)|![](./data/forest/cdmc317.jpg)|\n",
    "\n",
    "The problem is realively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several differnet possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Pre - Trained Model\n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "```\n",
    "\n",
    "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
    "\n",
    "```python\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
    "\n",
    "```python\n",
    "x = res.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(res.input, predictions)\n",
    "```\n",
    "\n",
    "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "Steps to complete assignment: \n",
    "1. Load in Image Data into numpy arrays (`X`) \n",
    "2. Create a `y` for the labels\n",
    "3. Train your model with pretrained layers from resnet\n",
    "4. Report your model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load in Data\n",
    "\n",
    "![skimage-logo](https://scikit-image.org/_static/img/logo.png)\n",
    "\n",
    "Check out out [`skimage`](https://scikit-image.org/) for useful functions related to processing the images. In particular checkout the documentation for `skimage.io.imread_collection` and `skimage.transform.resize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "forests = skimage.io.imread_collection(\"data//forest//*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data//forest\\\\art114.jpg',\n",
       " 'data//forest\\\\bost98.jpg',\n",
       " 'data//forest\\\\bost100.jpg',\n",
       " 'data//forest\\\\bost101.jpg',\n",
       " 'data//forest\\\\bost102.jpg',\n",
       " 'data//forest\\\\bost103.jpg',\n",
       " 'data//forest\\\\bost190.jpg',\n",
       " 'data//forest\\\\cdmc12.jpg',\n",
       " 'data//forest\\\\cdmc101.jpg',\n",
       " 'data//forest\\\\cdmc271.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forests.files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 256, 256, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forests=forests.concatenate()\n",
    "forests.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 256, 256, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mountains = skimage.io.imread_collection(\"data//mountain//*.jpg\")\n",
    "mountains = mountains.concatenate()\n",
    "mountains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 256, 256, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((forests, mountains))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0] * 328 + [1] * 374)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 85,  89,  62],\n",
       "        [ 96, 100,  75],\n",
       "        [ 83,  82,  61],\n",
       "        ...,\n",
       "        [ 62,  65,  48],\n",
       "        [ 29,  33,  18],\n",
       "        [ 52,  56,  41]],\n",
       "\n",
       "       [[ 97, 101,  74],\n",
       "        [128, 132, 107],\n",
       "        [119, 118,  97],\n",
       "        ...,\n",
       "        [118, 121, 104],\n",
       "        [ 87,  91,  76],\n",
       "        [ 72,  76,  61]],\n",
       "\n",
       "       [[ 97, 101,  74],\n",
       "        [141, 145, 120],\n",
       "        [121, 120,  99],\n",
       "        ...,\n",
       "        [ 76,  80,  63],\n",
       "        [ 88,  92,  77],\n",
       "        [ 91,  95,  80]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[120, 125,  58],\n",
       "        [163, 168, 101],\n",
       "        [174, 179, 112],\n",
       "        ...,\n",
       "        [162, 166, 107],\n",
       "        [171, 175, 116],\n",
       "        [126, 130,  71]],\n",
       "\n",
       "       [[162, 167, 101],\n",
       "        [188, 193, 127],\n",
       "        [194, 201, 134],\n",
       "        ...,\n",
       "        [138, 142,  83],\n",
       "        [161, 164, 107],\n",
       "        [131, 134,  77]],\n",
       "\n",
       "       [[130, 135,  71],\n",
       "        [130, 135,  71],\n",
       "        [124, 130,  66],\n",
       "        ...,\n",
       "        [116, 120,  61],\n",
       "        [138, 141,  84],\n",
       "        [102, 105,  48]]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For optimal processing, we scale the values down by 255 to set them between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_t = X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We instantialize a pre-trained instance of the ResNet model. We remove the fully-connected layers so that we can train those layers on our own data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I'm setting the layers to non-trainable -- for computational purposes, we want to use the learned parameters without updating them in future passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "We then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "#(fancy flattening)\n",
    "x = Dense(1024, activation = 'relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(resnet.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/20\n",
      "561/561 [==============================] - 15s 27ms/sample - loss: 0.5232 - accuracy: 0.8984 - val_loss: 7.0171 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.1171 - accuracy: 0.9697 - val_loss: 0.9087 - val_accuracy: 0.6525\n",
      "Epoch 3/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0222 - accuracy: 0.9947 - val_loss: 0.7800 - val_accuracy: 0.7021\n",
      "Epoch 4/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0183 - accuracy: 0.9947 - val_loss: 0.2809 - val_accuracy: 0.8794\n",
      "Epoch 5/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0164 - accuracy: 0.9947 - val_loss: 1.2282 - val_accuracy: 0.6596\n",
      "Epoch 6/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.6511 - val_accuracy: 0.7872\n",
      "Epoch 7/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.9245 - val_accuracy: 0.7234\n",
      "Epoch 8/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.2485 - val_accuracy: 0.6879\n",
      "Epoch 9/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.8863 - val_accuracy: 0.7589\n",
      "Epoch 10/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 0.9982 - val_loss: 0.6739 - val_accuracy: 0.8085\n",
      "Epoch 11/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0320 - accuracy: 0.9875 - val_loss: 0.5341 - val_accuracy: 0.8156\n",
      "Epoch 12/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6997 - val_accuracy: 0.8014\n",
      "Epoch 13/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.1340 - val_accuracy: 0.9574\n",
      "Epoch 14/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.4065 - val_accuracy: 0.8511\n",
      "Epoch 15/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7206 - val_accuracy: 0.7801\n",
      "Epoch 16/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.8039 - val_accuracy: 0.7801\n",
      "Epoch 17/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.9433\n",
      "Epoch 18/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5773 - val_accuracy: 0.8227\n",
      "Epoch 19/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.8652\n",
      "Epoch 20/20\n",
      "561/561 [==============================] - 5s 9ms/sample - loss: 4.8690e-04 - accuracy: 1.0000 - val_loss: 0.4036 - val_accuracy: 0.8723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d38449fba8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The final model yields a validation accuracy of 87.2% (and is perfect on the test data set). The best model had a validation accuracy of 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/702 [===>..........................] - ETA: 8s - loss: 2.6345e-05 - accuracy: 1.0000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7528810278402177e-06, 1.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y, batch_size=10, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Custom CNN Model\n",
    "\n",
    "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 60, 60, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 230400)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               29491328  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 29,564,289\n",
      "Trainable params: 29,564,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='nadam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/25\n",
      "561/561 [==============================] - 3s 5ms/sample - loss: 0.0231 - accuracy: 0.9947 - val_loss: 4.1981 - val_accuracy: 0.5461\n",
      "Epoch 2/25\n",
      "561/561 [==============================] - 3s 5ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 4.3557 - val_accuracy: 0.5887\n",
      "Epoch 3/25\n",
      "561/561 [==============================] - 2s 4ms/sample - loss: 0.0311 - accuracy: 0.9964 - val_loss: 5.8972 - val_accuracy: 0.4823\n",
      "Epoch 4/25\n",
      "561/561 [==============================] - 3s 5ms/sample - loss: 0.0124 - accuracy: 0.9982 - val_loss: 3.2247 - val_accuracy: 0.6099\n",
      "Epoch 5/25\n",
      "561/561 [==============================] - 3s 5ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.5944 - val_accuracy: 0.6099\n",
      "Epoch 6/25\n",
      "561/561 [==============================] - 3s 4ms/sample - loss: 0.0050 - accuracy: 0.9982 - val_loss: 3.4116 - val_accuracy: 0.6028\n",
      "Epoch 7/25\n",
      "561/561 [==============================] - 3s 4ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.7880 - val_accuracy: 0.6028\n",
      "Epoch 8/25\n",
      "561/561 [==============================] - 3s 4ms/sample - loss: 0.0030 - accuracy: 0.9982 - val_loss: 3.7368 - val_accuracy: 0.6028\n",
      "Epoch 9/25\n",
      "561/561 [==============================] - 3s 4ms/sample - loss: 8.5692e-04 - accuracy: 1.0000 - val_loss: 3.9184 - val_accuracy: 0.6099\n",
      "Epoch 10/25\n",
      "561/561 [==============================] - 3s 4ms/sample - loss: 0.0232 - accuracy: 0.9964 - val_loss: 4.0314 - val_accuracy: 0.5461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d38eaf1dd8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = EarlyStopping(monitor='val_accuracy', min_delta=.01, patience=6)\n",
    "model.fit(X, y, epochs=25, validation_split=.2, callbacks=[stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Custom CNN Model with Image Manipulations\n",
    "## *This a stretch goal, and it's relatively difficult*\n",
    "\n",
    "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Check out these resources to help you get started: \n",
    "\n",
    "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
    "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 20 steps\n",
      "Epoch 1/25\n",
      "20/20 [==============================] - 9s 474ms/step - loss: 0.1440 - accuracy: 0.9436\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 9s 459ms/step - loss: 0.1273 - accuracy: 0.9530\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 9s 462ms/step - loss: 0.1544 - accuracy: 0.9373\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 13s 644ms/step - loss: 0.1136 - accuracy: 0.9577\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 9s 467ms/step - loss: 0.1290 - accuracy: 0.9469\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 10s 481ms/step - loss: 0.1467 - accuracy: 0.9420\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 9s 475ms/step - loss: 0.1438 - accuracy: 0.9641\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 9s 457ms/step - loss: 0.1226 - accuracy: 0.9453\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 9s 462ms/step - loss: 0.1417 - accuracy: 0.9545\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 9s 472ms/step - loss: 0.1177 - accuracy: 0.9514\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 0.1272 - accuracy: 0.9545\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 10s 481ms/step - loss: 0.1311 - accuracy: 0.9483\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 10s 483ms/step - loss: 0.1246 - accuracy: 0.9516\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 10s 483ms/step - loss: 0.1270 - accuracy: 0.9467\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 9s 473ms/step - loss: 0.1243 - accuracy: 0.9545\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 9s 470ms/step - loss: 0.1246 - accuracy: 0.9467\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 0.1081 - accuracy: 0.9639\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 10s 486ms/step - loss: 0.1536 - accuracy: 0.9436\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 9s 473ms/step - loss: 0.1237 - accuracy: 0.9483\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 9s 471ms/step - loss: 0.1227 - accuracy: 0.9451\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 12s 615ms/step - loss: 0.1128 - accuracy: 0.9639\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 12s 600ms/step - loss: 0.1217 - accuracy: 0.9451\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 11s 528ms/step - loss: 0.1136 - accuracy: 0.9624\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 12s 599ms/step - loss: 0.1009 - accuracy: 0.9530\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 13s 671ms/step - loss: 0.1417 - accuracy: 0.9451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d38eb1e1d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(X, y, batch_size=32), steps_per_epoch=20, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "2020-03-24 17:01:58.094134: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
    "\n",
    "Per documentation, make the below change to avoid error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 21.9375 steps\n",
      "Epoch 1/25\n",
      "22/21 [==============================] - 12s 527ms/step - loss: 0.0993 - accuracy: 0.9573\n",
      "Epoch 2/25\n",
      "22/21 [==============================] - 11s 500ms/step - loss: 0.1195 - accuracy: 0.9501\n",
      "Epoch 3/25\n",
      "22/21 [==============================] - 11s 497ms/step - loss: 0.1306 - accuracy: 0.9501\n",
      "Epoch 4/25\n",
      "22/21 [==============================] - 12s 545ms/step - loss: 0.1249 - accuracy: 0.9516\n",
      "Epoch 5/25\n",
      "22/21 [==============================] - 13s 578ms/step - loss: 0.1348 - accuracy: 0.9473\n",
      "Epoch 6/25\n",
      "22/21 [==============================] - 11s 519ms/step - loss: 0.1234 - accuracy: 0.9573\n",
      "Epoch 7/25\n",
      "22/21 [==============================] - 12s 527ms/step - loss: 0.1289 - accuracy: 0.9615\n",
      "Epoch 8/25\n",
      "22/21 [==============================] - 12s 526ms/step - loss: 0.1086 - accuracy: 0.9544\n",
      "Epoch 9/25\n",
      "22/21 [==============================] - 11s 516ms/step - loss: 0.0963 - accuracy: 0.9644\n",
      "Epoch 10/25\n",
      "22/21 [==============================] - 11s 520ms/step - loss: 0.0881 - accuracy: 0.9658\n",
      "Epoch 11/25\n",
      "22/21 [==============================] - 12s 539ms/step - loss: 0.0767 - accuracy: 0.9729\n",
      "Epoch 12/25\n",
      "22/21 [==============================] - 11s 512ms/step - loss: 0.1045 - accuracy: 0.9558\n",
      "Epoch 13/25\n",
      "22/21 [==============================] - 11s 513ms/step - loss: 0.0883 - accuracy: 0.9672\n",
      "Epoch 14/25\n",
      "22/21 [==============================] - 11s 520ms/step - loss: 0.0953 - accuracy: 0.9630\n",
      "Epoch 15/25\n",
      "22/21 [==============================] - 12s 540ms/step - loss: 0.0927 - accuracy: 0.9630\n",
      "Epoch 16/25\n",
      "22/21 [==============================] - 11s 511ms/step - loss: 0.0974 - accuracy: 0.9587\n",
      "Epoch 17/25\n",
      "22/21 [==============================] - 11s 502ms/step - loss: 0.0999 - accuracy: 0.9701\n",
      "Epoch 18/25\n",
      "22/21 [==============================] - 11s 523ms/step - loss: 0.0962 - accuracy: 0.9558\n",
      "Epoch 19/25\n",
      "22/21 [==============================] - 12s 538ms/step - loss: 0.0660 - accuracy: 0.9672\n",
      "Epoch 20/25\n",
      "22/21 [==============================] - 12s 533ms/step - loss: 0.0948 - accuracy: 0.9587\n",
      "Epoch 21/25\n",
      "22/21 [==============================] - 13s 591ms/step - loss: 0.1304 - accuracy: 0.9501\n",
      "Epoch 22/25\n",
      "22/21 [==============================] - 12s 523ms/step - loss: 0.1039 - accuracy: 0.9601\n",
      "Epoch 23/25\n",
      "22/21 [==============================] - 11s 507ms/step - loss: 0.0723 - accuracy: 0.9701\n",
      "Epoch 24/25\n",
      "22/21 [==============================] - 11s 516ms/step - loss: 0.0846 - accuracy: 0.9701\n",
      "Epoch 25/25\n",
      "22/21 [==============================] - 11s 507ms/step - loss: 0.0692 - accuracy: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d38ec80240>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(X, y, batch_size=32), steps_per_epoch=len(X)/32, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "How to tell if this is actually working?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "# Resources and Stretch Goals\n",
    "\n",
    "Stretch goals\n",
    "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
    "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
    "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
    "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
    "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
    "\n",
    "Resources\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
    "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
    "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
    "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
    "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3 (Python3)",
   "language": "python",
   "name": "u4-s3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
