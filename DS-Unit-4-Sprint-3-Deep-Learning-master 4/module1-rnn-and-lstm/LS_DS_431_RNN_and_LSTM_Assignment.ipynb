{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\nProject Gutenberg’s The Complete Works of William Shakespeare, by William\\nShakespeare\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org.  If you are not located in the United States, you’ll\\nhave to check the laws of the country where you are l'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} \n",
    "\n",
    "# Here's where we'd replace text with tokenizing text if we wanted to do the dict \n",
    "# over words, not just characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1114621\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#sequences = sequences[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# embedding if words model. maybe should embed here anyways? how would this affect things?\n",
    "model.add(LSTM(512, input_shape=(maxlen, len(chars)), dropout=.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "from tensorflow.keras.layers import CuDNNLSTM\n",
    "\n",
    "The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1114621 samples\n",
      "Epoch 1/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 2.7559\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"r- alas! you see how 'tis- a little o'erparted. Bu\"\n",
      "r- alas! you see how 'tis- a little o'erparted. BuHKSè. S\n",
      "AL RALEO.\n",
      "WheGuts of biung te aich—u ane mer\n",
      "n  ia poom sf'e you  wome\n",
      "itest ard Padey f ou gaved.  WAtS BERSESGHeSEREByUMHAMAN.\n",
      "Whereard, youl  agras  c youg;\n",
      "Ua nit tor spric.  e tof orirher  titibe seo tiengr?\n",
      "PIMLTR.\n",
      "No mempsteu ndlqumsngstelf an here shar. hon whise hoolq;ist bn   he tara s\n",
      "    BrEs.. S IORDUSO.\n",
      "Wher bware ed  houch sue\n",
      "t y lorrhbe duque: oo lothay thisves ee’b rat, t\n",
      "1114621/1114621 [==============================] - 237s 212us/sample - loss: 2.7558\n",
      "Epoch 2/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 2.2982\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"\n",
      "ALICE.\n",
      "De nails, _madame._\n",
      "\n",
      "KATHARINE.\n",
      "De nails, \"\n",
      "\n",
      "ALICE.\n",
      "De nails, _madame._\n",
      "\n",
      "KATHARINE.\n",
      "De nails, I me ler?\n",
      "  QRENBER. Co sablit no, thet with tood lesten.\n",
      "    I lave to bushakedsterice; trie, hou steem; me,\n",
      "    esheis tith ther surhmand\n",
      " ane mist encart antire, in he hemr, sor; ir dostalowhead\n",
      "'re conse oa muster uck oer\n",
      "sy yow fit so swincy of ugentissaded io, Heail ont; that levene?\n",
      "    BI hmycth the kend sharr Ioden scasis?\n",
      "Shap mary aavim, all jece. Yow hongy; a daritorye, the bathen, my \n",
      "1114621/1114621 [==============================] - 233s 209us/sample - loss: 2.2981\n",
      "Epoch 3/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 2.1641\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"e,\n",
      "Yet bury him as a prince.\n",
      "\n",
      "GUIDERIUS.\n",
      "Pray you \"\n",
      "e,\n",
      "Yet bury him as a prince.\n",
      "\n",
      "GUIDERIUS.\n",
      "Pray you come vontoms of   ogin, on pesp. We’ chuce pied hit tham the reg.\n",
      "    And then harch beive the ste te me.\n",
      "\n",
      "GAVIES\n",
      "\n",
      "Fore, llave as, thas cow on the qome.\n",
      "  KUNG YOLE. Betcead fo tho biff the, sumppe be tre lis, Gaven; onay your wu cons tho nme. I\n",
      "beld fith to me my that's withemun\n",
      "                               Exiun ScRENTISI RLANDPRbSt And sterone PuEnno Hess, varm.\n",
      "\n",
      "IRGTNMELANDE.\n",
      "Ale than your w\n",
      "1114621/1114621 [==============================] - 233s 209us/sample - loss: 2.1641\n",
      "Epoch 4/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 2.0698\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"st false speaking\n",
      "Was this upon myself. What I am \"\n",
      "st false speaking\n",
      "Was this upon myself. What I am is conest,\n",
      "Ar ihe machimnsond come lostss art you to thit.—Ahastyour connspontd\n",
      "Go bow’d pat prend!\n",
      "\n",
      "LARGAGL.\n",
      "Gauksis cou caless in tonecalion.\n",
      "\n",
      "HARGURB.\n",
      "Herew, seritht.\n",
      "\n",
      "PARTIAN.\n",
      "Oy thir [_thel har, braied the farr sar, ane I'smart,\n",
      "Ald pasinimost fell molriops gatea\n",
      "Det is the nobrso of indrains.\n",
      "Bas ba you mpr, timp the eromistnatre comp,\n",
      "Lame hape, ank.]\n",
      "\n",
      "Entit CAre.\n",
      "\n",
      "\n",
      "SCENE II. Soare aliahing\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 2.0698\n",
      "Epoch 5/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.9979\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"bservance,\n",
      "    All humbleness, all patience, and i\"\n",
      "bservance,\n",
      "    All humbleness, all patience, and in rono fair\n",
      "    And lift of sppitily unstor'I heave that of the shold\n",
      "Fore extenires\n",
      "    Resceef the leest faterebly herven hin in\n",
      "The nesirnagain and younigy love. To you shist, To deapt, it Bagunt cumas out King eace bas\n",
      "    And thee mige a denthtets of the are.    SENPERAF Do a yout fore Casthese\n",
      "\n",
      "                  H.                         To wislefale.\n",
      "                                   Exto\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.9979\n",
      "Epoch 6/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.9407\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"        SCENE VII.\n",
      "             Field of battle be\"\n",
      "        SCENE VII.\n",
      "             Field of battle began me,\n",
      "    Sir, I hume of buros, on this fougs?\n",
      "  STRPHISA. Haw now be the you, foo shail love ham looe,\n",
      "    Which poai stander on offyaly, Nod, I coules;\n",
      "    As though I wike withialkand courter'd,\n",
      "    Thes yould belfive it who ehrs thy wern.\n",
      "    Thissgune and sUndre's gidy trove so, whie\n",
      "    As go sid as mutina, mutyentefires te wees.\n",
      "  BRALAR. May you spay'd put in you so an. To have wee we, I\n",
      "1114621/1114621 [==============================] - 239s 214us/sample - loss: 1.9407\n",
      "Epoch 7/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.8924\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"reportest him to be.\n",
      "  SPEED. Why, thou whoreson a\"\n",
      "reportest him to be.\n",
      "  SPEED. Why, thou whoreson a better wroph't trowas betite\n",
      "Come by we sirn aghing herse,\n",
      "To doow not lide the foul ald virtuben wind\n",
      "After wis geale veryup! what a, the mbcked it:\n",
      "Unth woldot lost onfharch!\n",
      "\n",
      "LEPALENTA.\n",
      "[hat I talbst not that withally, [Grive I ammas. I would you wir\n",
      "ot is not mounts, I masonsembour? What be wall awoo;\n",
      "foosion crinting manes it night-te to me.\n",
      "\n",
      "OLAVYO.\n",
      "West they to come apsearem'd;\n",
      "now toou he\n",
      "1114621/1114621 [==============================] - 235s 211us/sample - loss: 1.8924\n",
      "Epoch 8/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.8530\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \" point of human skill,\n",
      "Reason becomes the marshal \"\n",
      " point of human skill,\n",
      "Reason becomes the marshal no yould handly: Boung lay that\n",
      "I am asalse charmors. This faws of fool! Ro would\n",
      "  Le very dears in exese,\n",
      "  MERMANI. Pare no moge Serlant,\n",
      "with sorrece-gentl? and I hade bet as mind again?\n",
      "\n",
      "JALE.\n",
      "No do.\n",
      "\n",
      "KING.\n",
      "And the heart, bas sprants you Groteeth\n",
      "R down that stirs, in yours mady may tryest\n",
      "If shreias quper iols instrownts Greak up hir. Woat some? I small\n",
      "Mecquefes’d though ever sat, and hose \n",
      "1114621/1114621 [==============================] - 233s 209us/sample - loss: 1.8530\n",
      "Epoch 9/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.8164\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"k in\n",
      "          a chair. Enter TALBOT and BURGUNDY \"\n",
      "k in\n",
      "          a chair. Enter TALBOT and BURGUNDY LENUMORD MAPGORE, mirdrellio naitery,\n",
      "  A Stake of sure bue mertery stifel she hands.\n",
      "    Wall of Eharl's ave moint. What, oay\n",
      "give core\n",
      "    that, 'aw itsit. My satters lentles, as I dreap a soverly.\n",
      "\n",
      "NIOST.\n",
      "Lasty away, Rome to lee me; where is her to hopsanding\n",
      "    His grave, befaire them for you, I thou to acquantEice will he waugt about!\n",
      "    For I’ll rads Offactly, I had suat his blaed,\n",
      "    Cal\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.8164\n",
      "Epoch 10/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.7877\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"r lordship\n",
      "    To meet his Grace just distance 'tw\"\n",
      "r lordship\n",
      "    To meet his Grace just distance 'twa dispeth n my,\n",
      "  What ofter that give thy mote againtt her wits\n",
      "      and thee; So has mease the sceaice, huphin a stalothme to ETentims. A \n",
      "H     [With a astron in Fount\n",
      "\n",
      "ANTIPUS, here hath prizes\n",
      "\n",
      "CAEDER\n",
      "   CLAUCDo ATROWKE 'OWabout no men ef rich, approus'd\n",
      "    You our a'stais'd and fiod fortwonning Truth\n",
      "    Thy desert I bount the King's purotee cost.\n",
      "  BUYKL. Here wild ame for your pray not n\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.7877\n",
      "Epoch 11/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.7607\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \" he parted from me,\n",
      "    To bind him to remember my\"\n",
      " he parted from me,\n",
      "    To bind him to remember my fance;\n",
      "    Then, where is too: forth' hare-to this dide-\n",
      "  FALETAFF. Blist's deot rishors My appedoo this; therefore, she but grod\n",
      "    u anvest of bone if Shildis; thou sot the caumtet\n",
      "    The yemppor's cullien of the haster's hind,\n",
      "    Une crowing of Losemmurness gloss,\n",
      "    Kinely, good lud't on there semptwits are thee.\n",
      "\n",
      "BERTR.\n",
      "He ened the Queen! A yor prume’s uncaspl.\n",
      "\n",
      "HORLANIS.\n",
      "Hepen shepe, g\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.7607\n",
      "Epoch 12/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.7359\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \"ets now\n",
      "    That strew the green lap of the new co\"\n",
      "ets now\n",
      "    That strew the green lap of the new covesth\n",
      "    The gayslot witer dim affaity in show,\n",
      "    Unled meet from hirsh of fey, the great fall\n",
      "    These formurion in by doow. And, cenora to mint\n",
      "    Is ifly Jaly shall carile him, mer’s mueth?\n",
      "  QUEEN KLIZHBERE. You say nob Coesia to you hear, that corre\n",
      "tone\n",
      "\n",
      "Haw tot me do bon out oppod othe ride.\n",
      "\n",
      "IAGO.\n",
      "That if this jumber livew the cailod, fresc’e,\n",
      "Or fautle wexts, and you ow, make her par\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.7359\n",
      "Epoch 13/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.7162\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"ent merit of the unworthy takes,\n",
      "When he himself m\"\n",
      "ent merit of the unworthy takes,\n",
      "When he himself my fountina in his lany.\n",
      "\n",
      "ATOALO.\n",
      "Doth your from sanowhere?\n",
      "\n",
      "OTHELLO.\n",
      "I gry long excipe what I then you in the lord?\n",
      "\n",
      "FLOAIA.\n",
      "Why, do, you arast?\n",
      "\n",
      "LONT.\n",
      "I hage you ding of cold are entrity.\n",
      "\n",
      "WALONT.\n",
      "It is what I oe her sape,\n",
      "With you so herc \"f my buseray wise dreak.\n",
      "\n",
      "FRONS LARRETHERRANT.\n",
      "I move ud is meneven Brue a gintin ther.\n",
      "I’ll geve it in that dangsion so awaw’sth;\n",
      "COKIN MENCELSTE.\n",
      "Marries, m\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.7162\n",
      "Epoch 14/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.6968\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"cold fear, that mean and gentle all\n",
      "Behold, as may\"\n",
      "cold fear, that mean and gentle all\n",
      "Behold, as may her Ploan servy onch woman\n",
      "Dought and there spercily!”\n",
      "This is the cannattion beet bles youmace.\n",
      "I have think noing. Have net not sey the monner\n",
      "The name of Manoua, wherewars of this cerp\n",
      "Why almests thing, madres’ black for th' has? in this\n",
      "          com'l there or in't ftee love with eye;\n",
      "    And renery and perfords my doth not will de\n",
      "    O ployatonle? No, and thought he is at a’l sponehfur. B\n",
      "1114621/1114621 [==============================] - 238s 214us/sample - loss: 1.6968\n",
      "Epoch 15/15\n",
      "1114000/1114621 [============================>.] - ETA: 0s - loss: 1.6786\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \"ncerns the Turk than Rhodes,\n",
      "So may he with more f\"\n",
      "ncerns the Turk than Rhodes,\n",
      "So may he with more focmz; we’tlee home itact.\n",
      "Wh’ secation better death of me a crays'd:\n",
      "Hepengs art an eye love thay I quan, burn mewn\n",
      "As tites the never. Gut her onner,\n",
      "His ongoyelerwers by that hast whone ooe,\n",
      "For sor’d ahe moody of line, and therein heapen\n",
      "Frou taked men oo kent hences, coupit bit moth,\n",
      "We that enowns: in here pog-tions! all the secre wore a drumio with you;\n",
      "the\n",
      "feelet, men of her prese. Rishable\n",
      "1114621/1114621 [==============================] - 234s 210us/sample - loss: 1.6786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218a54bae48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=2000,\n",
    "          epochs=15,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Use partially trained model to train more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1114623\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1114623 samples\n",
      "Epoch 1/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.4279\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" our seeing, goe we hence,\n",
      "Right joyfull\"\n",
      " our seeing, goe we hence,\n",
      "Right joyfull a pratt, yas nge; ghell\n",
      "     Thes soor my uame in wole hent cooi wid.\n",
      "\n",
      "THKOGA.\n",
      "PSor arg'grelt\n",
      "    ir is  mout a’d thev din cringom oulitis; ary;  n arith m srarsels? Sou’s mo, ank hateatis qury;\n",
      "\n",
      " _UHICEL._]athy, aWsme wov'e sblbt bar; ar Myse nitherd INisks'nd,\n",
      "\n",
      "    houzheclsenth woucurt, orv, thange tit\n",
      "yam norm whall ofh, wher, peary;\n",
      "hy oul as whing aulesi he sesto  numgh,\n",
      " T  nin to sa fase \n",
      "1114623/1114623 [==============================] - 51s 46us/sample - loss: 2.4278\n",
      "Epoch 2/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.3261\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"on the instant that she was accus’d,\n",
      "   \"\n",
      "on the instant that she was accus’d,\n",
      "    shit mbelir ly. Lelly ham prany 'rons,\n",
      "And fit wivtime.\n",
      "Mike Vhat the motbly doovesof,\n",
      "Fod frriule, bean}aet hiy ooxterd Sat Gorte\n",
      "\n",
      "KIUOE.\n",
      "Thus monchy ustmith, ardin carefanteuchen thour\n",
      "\n",
      "AChEt. Sovve, bevouding celfior I se toutour; I hastique;\n",
      "d atrrse thesvin ot a pilanistes fuy mice wuglatissst net.\n",
      "  BAUADEL. Fat, I arg.\n",
      "\n",
      "TERTERVET.\n",
      "We yo sere. IHithy, rnfemy bot rive ing til to,\n",
      "home. UUWhI\n",
      "1114623/1114623 [==============================] - 47s 42us/sample - loss: 2.3261\n",
      "Epoch 3/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.2664\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \" Or may we cram\n",
      "Within this wooden O the\"\n",
      " Or may we cram\n",
      "Within this wooden O theurep fat I as treur;\n",
      "Whetrivendain bet rencoritt will an awerin. I cone dote\n",
      "    I aeues, manjupst int d wad? this frimn bp widrowst, ardzee; I dils art dy, lidf, mase, twarl for corad-swaly,\n",
      "And she plodce to taom toen amprvand;\n",
      "I ald sade. Ho ou 'oj!       Ta the plow; be fare me soay coreat;\n",
      "The ablen’d to cegly ciles anes.   tondting to lagayxmake.\n",
      "    Extut and in foresgich-emory, doul f olle\n",
      "1114623/1114623 [==============================] - 48s 43us/sample - loss: 2.2665\n",
      "Epoch 4/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.2238\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"\n",
      "    For he's inclin'd as is the ravenou\"\n",
      "\n",
      "    For he's inclin'd as is the ravenoust of dobjevid.\n",
      "    Fatr what heave I  or have rkeal his ookl Romere?\n",
      "  PSENINLI. To fwall your se my pase deal-ect man that ut towen; is tidg will fidses\n",
      " n      u wrtmerdy, Houre, by loran sofentsou murd thails,\n",
      "\n",
      " T  CUKES. Alk baiet’s macn spoy wasors the this hass\n",
      "    Iigh, he quel a de? wher and iof in io?\n",
      "  TERTAVINE. Alt us we hing, nom see’shand be\n",
      "    Helf at stoyrumiou vur the ome, swill\n",
      "1114623/1114623 [==============================] - 51s 45us/sample - loss: 2.2238\n",
      "Epoch 5/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.1886\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"sir.\n",
      "\n",
      "FLORIZEL.\n",
      "So call it: but it does \"\n",
      "sir.\n",
      "\n",
      "FLORIZEL.\n",
      "So call it: but it does the notyem chivr fibe.\n",
      "    Fne Phole, th  lend you afort me cas erxd?\n",
      "  LION] L, the mant;\n",
      "So murcess ounk tho Lave Then the well mig. not the fome beee your she whim seng\n",
      "    As with mour Loatiee Mare hon theredbreven,\n",
      "    giest will us now I 'hit thiksstrnm a dive\n",
      "    Oh thecinde\n",
      "fre eratry, boncless wire ho did\n",
      "this nlabcings-shy mine\n",
      "To h’s ersplamy bus that I'll youn sheil spn' the congleteno\n",
      "1114623/1114623 [==============================] - 54s 48us/sample - loss: 2.1885\n",
      "Epoch 6/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.1606\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"ate king!\n",
      "    But, lords, we hear this f\"\n",
      "ate king!\n",
      "    But, lords, we hear this fok, a may I with\n",
      "\n",
      "A WELPAEE ATHWHGET.\n",
      "Have aven the hiny of say gandano,\n",
      "    Forts\n",
      "loweshavers to kort, het thid thee\n",
      "    rigglvedinges that hethom. Whctghing! Do dend Hongetay mas sit I the brond?\n",
      "  RUTIRUS. Weus sor up oup, heftoest tho averr land;\n",
      "O, ters of whom for mart he kith an\n",
      "Bo to to golf a jevined, yis ood stabe the your meace to thee! ’somfot,\n",
      "    To beid'd’IHady feem hum.\n",
      "\n",
      "MERTINE.\n",
      "H\n",
      "1114623/1114623 [==============================] - 47s 42us/sample - loss: 2.1605\n",
      "Epoch 7/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.1365\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"the plaster.\n",
      "\n",
      "SEBASTIAN.\n",
      "Very well.\n",
      "\n",
      "ANT\"\n",
      "the plaster.\n",
      "\n",
      "SEBASTIAN.\n",
      "Very well.\n",
      "\n",
      "ANTOME.\n",
      "Dest you a tann thit to he detitson, ateave Ragentor\n",
      "  MANY. Bave, at myserilath, from rppy-tanegh arkst\n",
      "    Entee nog the aruth whe coppromay if Why.\n",
      "    Whoch on Kink'd hemy art wonden, do him Feresee,\n",
      "Nrcther More thear’dein is ef o cotr and Horl’' the Lone,\n",
      "The  tame goon tas ale bmes sut pheshe.\n",
      "Theissent of peacire feltes mose chearte\n",
      "\n",
      "                Exturt JUND IUCE BEMIAMUS]\n",
      "ILSIUDAS\n",
      "1114623/1114623 [==============================] - 50s 45us/sample - loss: 2.1365\n",
      "Epoch 8/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.1125\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"y life, my wife! O Imogen,\n",
      "Imogen, Imoge\"\n",
      "y life, my wife! O Imogen,\n",
      "Imogen, Imoge''t duit? _Ombel; of to feir.\n",
      "\n",
      "PHTRGULY.\n",
      "Nove becuntstshinswart apleane noch is lose tye\n",
      "\n",
      "  By sut knot breraning of Kays this all be\n",
      "En whar epon pould bawncour\n",
      "    Kengon froch works my cadlord penen reenceace by erve?\n",
      "Whith wechfe ye druingsain thet bence a daddods.\n",
      "    and enanown munketh yourthincest’s cale on itle\n",
      "    The quertelus an you conght bof the serr.\n",
      "  T HATOEBAR. Why,, fsig the use\n",
      "1114623/1114623 [==============================] - 55s 50us/sample - loss: 2.1125\n",
      "Epoch 9/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0946\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \" fail you not to do, as you will—”\n",
      "\n",
      "LODO\"\n",
      " fail you not to do, as you will—”\n",
      "\n",
      "LODOOLLK.\n",
      "F’r ice blang of your to bit,\n",
      "At the lang to nit you, geatul acking hone teem eart abcianot Keblesis,\n",
      "Whonkirt dow, “ade the iset the Lill’d's sake;\n",
      "    That in to the vecce he juck tser Fonge,\n",
      "And knot in to dik'd: Entem woll I har there is compon Prelory?\n",
      "\n",
      "DUTROIULA.\n",
      "Now, it not bearter,\n",
      "And hever who adieat, shy pough muat witb dour hark ontel bur.\n",
      "  KINBESTIT. Whicha takens thangthc ubla\n",
      "1114623/1114623 [==============================] - 49s 44us/sample - loss: 2.0946\n",
      "Epoch 10/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0774\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"S.\n",
      "Not much employment for you. You unde\"\n",
      "S.\n",
      "Not much employment for you. You under u groa sanf Blove\n",
      "mendary, me lays Cailly and a brirs?\n",
      "\n",
      "\n",
      "MESTHESTUS\n",
      "If for surenow she Toetwithbers gals\n",
      "    Tou would menitug: doray frem held in this\n",
      "A frind the dants clitebe, hadem, bethan uncen\n",
      "r begnius blas, my wat efflifk'l of thetrefle.\n",
      "    Kenture gubon ie now sweer not well with and by histet and leer\n",
      "\n",
      "\n",
      "GROLBIRS.\n",
      "I houlow, the pracedes; so. Hais by I march.\n",
      "  QUCENTUS. Me'tugeames, gi\n",
      "1114623/1114623 [==============================] - 47s 43us/sample - loss: 2.0774\n",
      "Epoch 11/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0607\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \"y.\n",
      "  CLAUDIO. O, hear me, Isabella.\n",
      "\n",
      "   \"\n",
      "y.\n",
      "  CLAUDIO. O, hear me, Isabella.\n",
      "\n",
      "                                Enter PERICS, GRES, I The cantletem Lith, would\n",
      "LAUCHAN eter\n",
      "sporn\n",
      "\n",
      "[Kien, seall;\n",
      "Ender pasaic.]\n",
      "                                     n              CRED NE \n",
      "  LLLOUCETPA dathage, hid cherven\n",
      "woo thou Causin of the regrt soch mats bly wer.\n",
      "\n",
      "ACHEWIIN.\n",
      "Wxit\n",
      "Noows we morente, fom to boge, worlein.\n",
      "A with had, a dronned I tan-otee, foy his deuss\n",
      "\n",
      "                          \n",
      "1114623/1114623 [==============================] - 47s 42us/sample - loss: 2.0607\n",
      "Epoch 12/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0472\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \"thing have these nothings,\n",
      "If this be no\"\n",
      "thing have these nothings,\n",
      "If this be not If rokg a dester and hay will\n",
      "Mast’d hith vare to do cortioto’s hear’t of I dram flo\n",
      "       all to for’e makn? Hf hacewad madiesh and be wind tand reap ous meat;\n",
      "Wor hees what ow that Purshop? Bet, he os chalm proon'd it the\n",
      "\n",
      "  _Eener willivace and virate commands]\n",
      "\n",
      "YORATIA.\n",
      "[_Exeunt._]\n",
      "\n",
      "SACCEST.\n",
      "Yet knodsal!\n",
      "  BEPBLLUK You Gy, a rake and gintio day for his nim.\n",
      "\n",
      " ECNESTAR. I dose slarw with him\n",
      "1114623/1114623 [==============================] - 51s 46us/sample - loss: 2.0472\n",
      "Epoch 13/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0337\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"ing up the letter._]\n",
      "\n",
      "GLOUCESTER.\n",
      "Why so\"\n",
      "ing up the letter._]\n",
      "\n",
      "GLOUCESTER.\n",
      "Why sown my Rispsw\n",
      "    And thoue sleforst apome and the cumpery bultiot voalt,     So will sy thou! ser, Pame it eir componn',\n",
      "    And the' nave he rich of the heare.\n",
      "    Sor mort! Lose co have me ard be I. ifffem to yey.\n",
      "                  294.\n",
      "  And,\n",
      "    His place not tortiey to ells latt so for.\n",
      "\n",
      " QUEXENT. How ung seven buriagiow.\n",
      "\n",
      "  [_Soeeirin.]\n",
      "\n",
      "\n",
      "       ACHORD Y     FFrod atonnvous with aisp\n",
      "Roal, t\n",
      "1114623/1114623 [==============================] - 46s 41us/sample - loss: 2.0337\n",
      "Epoch 14/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0214\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"es that men do leave\n",
      "Are hated most of t\"\n",
      "es that men do leave\n",
      "Are hated most of the Duge by l'll not the wurrn!\n",
      "The grod! She dead shall I fart menytd be will-mud\n",
      "    But gurttio' bifitif’s lirtyon, conetues\n",
      "Lasts knds'd with menno hem but for tone!\n",
      "\n",
      "BEORTOLIO.\n",
      "No ustarthtoses, Alke's you he halicelt.\n",
      "  FORACTES. Thes so mysen shall conctiey; my leake hear.\n",
      "  pounnd a jake fel. Loke cower this dindre\n",
      "    A did sweence own there nontets’t.\n",
      "\n",
      "ARKINIA.\n",
      "Leupetty,\n",
      "Radiy _ree? I buzo\n",
      "1114623/1114623 [==============================] - 55s 49us/sample - loss: 2.0214\n",
      "Epoch 15/15\n",
      "1114000/1114623 [============================>.] - ETA: 0s - loss: 2.0100\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \"h girdle you about\n",
      "    By the compulsion\"\n",
      "h girdle you about\n",
      "    By the compulsion as ase thy wenher:\n",
      "Sou a infer masuleventand\n",
      "\n",
      "But APIUCY. Rockoug' him; provort a’ll’d I dold of yim batace be hone a ffoles af ghes\n",
      " t ebearters your sayre thou hath extle to everomame,\n",
      " a   fame, a sainher\n",
      "    To combange, yer, fails or thise-beit?\n",
      "In wotchemy thill thy facier.\n",
      "The butter ad on\n",
      ".   I am nof thy shall ip non laws gle' dubjeed\n",
      "With sey thou meater; and and with toos ’ot obee; ol \n",
      "1114623/1114623 [==============================] - 56s 50us/sample - loss: 2.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc0a6ddc18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=2000,\n",
    "          epochs=15,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Reproduce Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next step is to create universal input length so that everything is the same dimension for our model. For this, we use the pad_sequences method from Keras preprocessing in order to cut sequences above the max length down to it and add 0s to bring sequences up to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 80)\n",
      "(25000, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each of our observations is now a vector with length 80, where each element in the vector is essentially a one-hot encoding over a 20000-word vocabulary. What the embedding layer does is vectorize all this down into dense vectors in n dimensions. (Although it will look like it's adding dimensions, as each element of x_train will now be a series of 80 n-dimensional vectors.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Conventional LSTM models only have one LSTM layer before passing it to a dense layer to get output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=.2, recurrent_dropout=.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 24s 953us/sample - loss: 0.5400 - accuracy: 0.7244 - val_loss: 0.3752 - val_accuracy: 0.8348\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 22s 864us/sample - loss: 0.3147 - accuracy: 0.8721 - val_loss: 0.3826 - val_accuracy: 0.8346\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 21s 839us/sample - loss: 0.2430 - accuracy: 0.9056 - val_loss: 0.3852 - val_accuracy: 0.8296\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 21s 822us/sample - loss: 0.2061 - accuracy: 0.9244 - val_loss: 0.4429 - val_accuracy: 0.8259\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 20s 816us/sample - loss: 0.1699 - accuracy: 0.9388 - val_loss: 0.4870 - val_accuracy: 0.8204\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 19s 778us/sample - loss: 0.1342 - accuracy: 0.9546 - val_loss: 0.5502 - val_accuracy: 0.8177\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 19s 773us/sample - loss: 0.1193 - accuracy: 0.9591 - val_loss: 0.6022 - val_accuracy: 0.8109\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 20s 789us/sample - loss: 0.1004 - accuracy: 0.9647 - val_loss: 0.5984 - val_accuracy: 0.8092\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 20s 790us/sample - loss: 0.0857 - accuracy: 0.9692 - val_loss: 0.6384 - val_accuracy: 0.8104\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 20s 802us/sample - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.6312 - val_accuracy: 0.8102\n"
     ]
    }
   ],
   "source": [
    "saved_model = model.fit(x_train, y_train,\n",
    "                       batch_size=400,\n",
    "                       epochs=10,\n",
    "                       validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(saved_model.history['loss'])\n",
    "plt.plot(saved_model.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1bce0b5d4087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3 (Python3)",
   "language": "python",
   "name": "u4-s3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
